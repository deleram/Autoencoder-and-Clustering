{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7725, 0.8902, 0.0863, 0.0353,\n",
      "        0.3255, 0.6549, 0.7765, 1.0000, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,\n",
      "        0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7686, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9725, 0.9294, 0.9059, 0.6039, 0.5647, 0.5020,\n",
      "        0.9922, 0.7647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.3098,\n",
      "        0.5725, 0.5725, 0.5725, 0.3804, 0.1725, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.2980, 0.9922, 0.7647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0196, 0.7529, 0.9647, 0.3569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.1294, 0.9922, 0.7451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.2980, 0.9608, 0.2824, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.7294, 0.7569, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0902, 0.9725, 0.3255, 0.1333, 0.1176, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.4941, 0.9922, 0.9412, 0.7490, 0.0706,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.2000, 0.3216, 0.4392, 0.6627, 0.9843, 0.9490, 0.3608, 0.0157,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.9333, 0.8902,\n",
      "        0.9961, 0.9961, 0.9961, 0.9961, 0.9765, 0.9843, 0.9608, 0.3529, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.2863,\n",
      "        0.3490, 0.6039, 0.5216, 0.3922, 0.2863, 0.1176, 0.9098, 0.8941, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686, 0.9922, 0.6235,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7490, 0.9412,\n",
      "        0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.9961,\n",
      "        0.7725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3843,\n",
      "        0.9765, 0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.8196, 0.9608, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9608, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0863, 0.9725, 0.2510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.2902, 0.8863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "dfPath = r'labeled_train_set.csv'\n",
    "trainData = pd.read_csv(dfPath)\n",
    "df = pd.DataFrame(trainData)\n",
    "df_2 = df.iloc[:, 1:].values\n",
    "#transform values to pytorch tensor\n",
    "tensor_data = torch.tensor(df_2)\n",
    "#make them have values between 0 to 1\n",
    "normalized_tensor_data = tensor_data.float() / 255.0\n",
    "\n",
    "print(normalized_tensor_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Make a loader to enable us to iterate with batches of 32\n",
    "loader = torch.utils.data.DataLoader(dataset = normalized_tensor_data, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design our autoencoder\n",
    "\n",
    "# 28*28 -> 200 ->100 -> 50 -> 20 -> 10 -> 20 -> 50 -> 100 -> 200 -> 28*28\n",
    "# Takes torch neural network as super set\n",
    "class AE(torch.nn.Module):\n",
    "# Inherits init from torch.nn\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Define encoder as sequence of layers you want and their activation functions\n",
    "    # We use ReLU as our activation function because it performed well in our last exercise, and \n",
    "    # It often performs well in neural networks. Additionally, due to its computational efficiency, it is a preferred choice\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "    # The decoder is the reverse of the encoder, and it has the same structure. \n",
    "    # Since we normalized the data, we use a sigmoid function at the end of the decoder to predict numbers within the range of [0, 1].\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200, 28 * 28),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    # We show the series of actions taken in our Autoencoder\n",
    "    # First we encode and then we decode\n",
    "    # The output is our answer\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AE()\n",
    "\n",
    "# Create an optimzer. we used adams as it's so widely used in nn\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,  weight_decay = 1e-8)\n",
    "\n",
    "# Beacuse our data is some how continues we use MSE as loss function\n",
    "\n",
    "loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/50, loss = 0.000053\n",
      "epoch : 10/50, loss = 0.000050\n",
      "epoch : 20/50, loss = 0.000048\n",
      "epoch : 30/50, loss = 0.000058\n",
      "epoch : 40/50, loss = 0.000053\n",
      "epoch : 50/50, loss = 0.000060\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features in loader:\n",
    "        # Reshape each batch\n",
    "        batch_features = batch_features.reshape(-1, 28*28)\n",
    "        \n",
    "        # Reset the gradients for the new computations\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the output of our encoder\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, batch_features)\n",
    "        \n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        loss += loss.item()\n",
    "    \n",
    "    # Compute the epoch training loss\n",
    "    loss = loss / len(loader)\n",
    "    #print loss for 10*i th epoch\n",
    "    if(epoch%10 == 9 or epoch == 0):\n",
    "    # Display the epoch training loss\n",
    "        print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectures and parameters are discussed in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = next(iter(loader))\n",
    "for i, item in enumerate(image):\n",
    "    # Reshape the array for plotting\n",
    "    item = item.reshape(-1, 28, 28)\n",
    "    \n",
    "    # Convert the tensor to a NumPy array\n",
    "    item_np = item.detach().numpy()\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(item_np[0])\n",
    "    plt.show()\n",
    "    \n",
    "    # Break out of the loop after plotting the first image\n",
    "    break\n",
    "\n",
    "for i, item in enumerate(model(image)):\n",
    "    item = item.reshape(-1, 28, 28)\n",
    "    \n",
    "    # Convert the tensor to a NumPy array\n",
    "    item_np = item.detach().numpy()\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(item_np[0])\n",
    "    plt.show()\n",
    "    break\n",
    "# As you can see, after using the encoder, we obtained a significantly improved representation of the digits.\n",
    "# It can be considered as a denoising process. \n",
    "# Therefore, utilizing the autoencoder before classification assists in extracting enhanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we wanna apply the encoder on our data\n",
    "import numpy as np\n",
    "\n",
    "# Encode the data\n",
    "encoded_data = model.encoder(normalized_tensor_data)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "encoded_data_np = encoded_data.detach().numpy()\n",
    "labels = df['label']\n",
    "\n",
    "# Concatenate encoded data and labels\n",
    "data_with_labels = np.hstack((labels.values.reshape(-1, 1), encoded_data_np))\n",
    "\n",
    "# Create a DataFrame\n",
    "column_names = ['label'] + [f'feature{i+1}' for i in range(encoded_data_np.shape[1])]\n",
    "df_encoded = pd.DataFrame(data_with_labels, columns=column_names)\n",
    "\n",
    "# Save as CSV\n",
    "df_encoded.to_csv('encoded_data.csv', index=False)\n",
    "\n",
    "# Now we have our new features in a new csv called encoded_data.csv and we will use it for classification\n",
    "# The advantages will be discussed in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# PART II\n",
    "\n",
    "# Import what's needed\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Load the labeled data\n",
    "df = pd.read_csv('encoded_data.csv')\n",
    "\n",
    "# Separate the features and labels\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df['label'].values\n",
    "best_score = 0\n",
    "for hidden_layer_size in [(100,), (100, 25), (80, 50, 20)]:\n",
    "    for activations in ['relu', 'logistic', 'tanh', 'identity']:\n",
    "        for learning_rate in ['constant','adaptive']:\n",
    "            print(i)\n",
    "            i = i + 1\n",
    "            mlp = MLPClassifier(hidden_layer_sizes = hidden_layer_size, activation = activations, learning_rate = learning_rate, max_iter=1000)\n",
    "            scores = cross_val_score(mlp, X, y, cv=5)\n",
    "            average_score = scores.mean()\n",
    "\n",
    "            if average_score > best_score:\n",
    "                best_score = average_score\n",
    "                best_params = {'hidden' : hidden_layer_size, 'act':activations, 'learn':learning_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501111111111111\n",
      "{'hidden': (100,), 'act': 'relu', 'learn': 'adaptive'}\n"
     ]
    }
   ],
   "source": [
    "print(best_score)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n"
     ]
    }
   ],
   "source": [
    "best_mlp = MLPClassifier(hidden_layer_sizes = (100,), activation = 'relu'\n",
    "                         , learning_rate = 'adaptive'\n",
    "                         , max_iter=1000)\n",
    "# Now it's time to train the best model on our training set and then test in on our test set\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# we should change our test data to a 10-dimension data using our encoder\n",
    "df = pd.read_csv('test_set.csv')\n",
    "\n",
    "# Separate the features and labels\n",
    "X_test1 = df.iloc[:, 1:].values\n",
    "y_test = df['label'].values\n",
    "\n",
    "\n",
    "#transform values to pytorch tensor\n",
    "tensor_X = torch.tensor(X_test1).float()/255.0\n",
    "X_encoded_test = model.encoder(tensor_X)\n",
    "X_test = X_encoded_test.detach().numpy()\n",
    "\n",
    "best_mlp.fit(X, y)\n",
    "print('Training done')\n",
    "\n",
    "y_pred = best_mlp.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mtx = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Test Data\n",
      "Accuracy :  0.9314\n",
      "[[ 954    0    1    1    0    7   11    0    2    4]\n",
      " [   0 1112    3    1    1    1    3    2   11    1]\n",
      " [   7    1  968   18    5    3    4    8   16    2]\n",
      " [   1    1   13  934    1   10    1    9   36    4]\n",
      " [   2    1    3    1  918    0   12    3    4   38]\n",
      " [  14    1    3   26    6  792    7    1   36    6]\n",
      " [  12    4    0    0   12   10  914    1    1    4]\n",
      " [   0    6   13    7    9    2    1  940    7   43]\n",
      " [   1    3    4   34    7   44    1    5  866    9]\n",
      " [   2    8    3   12   37    6    4    7   14  916]]\n",
      "For Train Data\n",
      "Accuracy :  0.9728333333333333\n",
      "[[ 954    0    1    1    0    7   11    0    2    4]\n",
      " [   0 1112    3    1    1    1    3    2   11    1]\n",
      " [   7    1  968   18    5    3    4    8   16    2]\n",
      " [   1    1   13  934    1   10    1    9   36    4]\n",
      " [   2    1    3    1  918    0   12    3    4   38]\n",
      " [  14    1    3   26    6  792    7    1   36    6]\n",
      " [  12    4    0    0   12   10  914    1    1    4]\n",
      " [   0    6   13    7    9    2    1  940    7   43]\n",
      " [   1    3    4   34    7   44    1    5  866    9]\n",
      " [   2    8    3   12   37    6    4    7   14  916]]\n"
     ]
    }
   ],
   "source": [
    "print(\"For Test Data\")\n",
    "print(\"Accuracy : \" , accuracy)\n",
    "print(confusion_mtx)\n",
    "print(\"For Train Data\")\n",
    "y_pred2 = best_mlp.predict(X)\n",
    "accuracy2 = accuracy_score(y, y_pred2)\n",
    "confusion_mtx2 = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy : \" , accuracy2)\n",
    "print(confusion_mtx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part III\n",
    "# because our data is so sparse we perform dimension reduction using autoencoders\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "df = pd.read_csv('test_set.csv')\n",
    "\n",
    "# Separate the features and labels\n",
    "X_test1 = df.iloc[:, 1:].values\n",
    "y_test = df['label'].values\n",
    "\n",
    "\n",
    "#transform values to pytorch tensor\n",
    "tensor_X = torch.tensor(X_test1).float()/255.0\n",
    "X_encoded_test = model.encoder(tensor_X)\n",
    "X_test = X_encoded_test.detach().numpy()\n",
    "\n",
    "\n",
    "# Step 2: Prepare your data\n",
    "# Assuming you have your data stored in a variable called 'data'\n",
    "\n",
    "# Step 3: Choose the number of clusters (K)\n",
    "num_clusters = 3\n",
    "\n",
    "# Step 4: Create the K-means model\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# Step 5: Train the model\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Step 6: Obtain cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Print the cluster labels\n",
    "print(\"Cluster Labels:\")\n",
    "print(cluster_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
